{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOBFEaoiCXNafzkcNPbqFgO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ea-analisisdatos/web_scraping_google_colab_bookstoscrape/blob/main/WebScraping_BeautifulSoup_bookstoscrape.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Web Scrarping con BeautifulSoup\n",
        "\n",
        "Fuentes de Datos:\n",
        "https://www.crummy.com/software/BeautifulSoup/bs4/doc/"
      ],
      "metadata": {
        "id": "62-IYujh82ot"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Página Web que debe ser scrapeadas: https://books.toscrape.com/catalogue/category/books_1/index.html\n"
      ],
      "metadata": {
        "id": "URTVdX8k-7S5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ZEDipx8L8A-u",
        "outputId": "4ed02213-a9a4-45d8-b729-8a48391e9004"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bs4\n",
            "  Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from bs4) (4.12.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->bs4) (2.5)\n",
            "Installing collected packages: bs4\n",
            "Successfully installed bs4-0.0.2\n"
          ]
        }
      ],
      "source": [
        "# Instalación de librerías\n",
        "!pip install bs4"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalación de librería para exportar el csv automaticamente al repositorio de Github\n",
        "!pip install GitPython"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "I3q-Nba-EEW-",
        "outputId": "71bc3235-6ec3-4459-e0d2-ffb2e8705ffb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting GitPython\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gitdb<5,>=4.0.1 (from GitPython)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, gitdb, GitPython\n",
            "Successfully installed GitPython-3.1.43 gitdb-4.0.11 smmap-5.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scrapeando la web de libros con la librería BeautifulSoup"
      ],
      "metadata": {
        "id": "sWTWPYp3EUYf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Puedes arreglar el código anterior para en la columna Nombre en el CSV no presente caracteres especiales como este: Full Moon over Noahâs Ark: An Odyssey to Mount Ararat and Beyond?\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# URL de la página a scrapear\n",
        "url = 'https://books.toscrape.com/index.html'\n",
        "\n",
        "# Hacer la petición a la página\n",
        "response = requests.get(url)\n",
        "\n",
        "# Crear un objeto BeautifulSoup\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# Encontrar la lista de categorías\n",
        "categorias_sidebar = soup.find('ul', class_='nav nav-list').find_all('li')[1:]\n",
        "\n",
        "# Crear listas para almacenar los datos\n",
        "nombres = []\n",
        "estrellas = []\n",
        "precios = []\n",
        "disponibilidad = []\n",
        "categorias = []\n",
        "\n",
        "# Iterar sobre cada categoría\n",
        "for categoria_item in categorias_sidebar:\n",
        "  # Extraer el nombre de la categoría\n",
        "  categoria = categoria_item.a.text.strip()\n",
        "\n",
        "  # Construir la URL de la página de la categoría\n",
        "  categoria_url = 'https://books.toscrape.com/' + categoria_item.a['href']\n",
        "\n",
        "  # Hacer la petición a la página de la categoría\n",
        "  categoria_response = requests.get(categoria_url)\n",
        "  categoria_soup = BeautifulSoup(categoria_response.text, 'html.parser')\n",
        "\n",
        "  # Encontrar todos los artículos de libros en la página de la categoría\n",
        "  libros = categoria_soup.find_all('article', class_='product_pod')\n",
        "\n",
        "  # Iterar sobre cada libro en la categoría\n",
        "  for libro in libros:\n",
        "    # Extraer el nombre del libro y limpiar caracteres especiales\n",
        "    nombre = libro.h3.a['title']\n",
        "    nombre_limpio = ''.join(c for c in nombre if c.isalnum() or c.isspace())\n",
        "    nombres.append(nombre_limpio)\n",
        "\n",
        "    # Extraer las estrellas\n",
        "    estrella = libro.p['class'][1]\n",
        "    estrellas.append(estrella)\n",
        "\n",
        "    # Extraer el precio\n",
        "    precio = libro.find('p', class_='price_color').text[2:]\n",
        "    precios.append(precio)\n",
        "\n",
        "    # Extraer la disponibilidad\n",
        "    disponible = libro.find('p', class_='instock availability').text.strip()\n",
        "    disponibilidad.append(disponible)\n",
        "\n",
        "    # Agregar la categoría al libro\n",
        "    categorias.append(categoria)\n",
        "\n",
        "# Crear un DataFrame de Pandas\n",
        "df = pd.DataFrame({\n",
        "  'Nombre': nombres,\n",
        "  'Estrellas': estrellas,\n",
        "  'Precio': precios,\n",
        "  'Disponibilidad': disponibilidad,\n",
        "  'Categoría': categorias\n",
        "})\n",
        "\n",
        "# Guardar el DataFrame en un archivo CSV\n",
        "df.to_csv('libros.csv', index=False)\n"
      ],
      "metadata": {
        "id": "6SUFbpEtCmcf"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exportando el archio libros.csv hacía el repositorio de Github"
      ],
      "metadata": {
        "id": "M9imuYldEnPM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from git import Repo\n",
        "\n",
        "# Repositorio de GitHub (reemplaza con tu repositorio)\n",
        "repo_url = 'https://github.com/ea-analisisdatos/web_scraping_google_colab_bookstoscrape.git'\n",
        "\n",
        "# Directorio local donde clonar el repositorio (si no existe, se creará)\n",
        "repo_dir = 'mi_repositorio'\n",
        "\n",
        "# Clonar el repositorio si no existe\n",
        "if not os.path.exists(repo_dir):\n",
        "  Repo.clone_from(repo_url, repo_dir)\n",
        "\n",
        "# Acceder al repositorio\n",
        "repo = Repo(repo_dir)\n",
        "\n",
        "# Agregar el archivo CSV al repositorio\n",
        "repo.index.add(['libros.csv'])\n",
        "\n",
        "# Hacer commit de los cambios\n",
        "repo.index.commit('Agregar archivo libros.csv')\n",
        "\n",
        "# Subir los cambios a GitHub\n",
        "origin = repo.remote(name='origin')\n",
        "origin.push()\n",
        "\n",
        "print('Archivo libros.csv subido a GitHub.')\n"
      ],
      "metadata": {
        "id": "i0-Y8UV1Dvbw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}